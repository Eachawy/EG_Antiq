# Prometheus Alerting Rules for Kemetra Stack

groups:
  # API Service Alerts
  - name: api_alerts
    interval: 30s
    rules:
      # API Service Down
      - alert: APIServiceDown
        expr: up{job="api"} == 0
        for: 2m
        labels:
          severity: critical
          service: api
        annotations:
          summary: "API service is down"
          description: "The API service has been down for more than 2 minutes."

      # High API Error Rate
      - alert: HighAPIErrorRate
        expr: rate(http_requests_total{job="api",status=~"5.."}[5m]) > 0.05
        for: 5m
        labels:
          severity: warning
          service: api
        annotations:
          summary: "High API error rate detected"
          description: "API is returning more than 5% 5xx errors in the last 5 minutes."

      # High API Response Time
      - alert: HighAPIResponseTime
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job="api"}[5m])) > 2
        for: 5m
        labels:
          severity: warning
          service: api
        annotations:
          summary: "High API response time"
          description: "95th percentile API response time is above 2 seconds."

  # Portal Service Alerts
  - name: portal_alerts
    interval: 30s
    rules:
      # Portal Service Down
      - alert: PortalServiceDown
        expr: up{job="portal"} == 0
        for: 2m
        labels:
          severity: critical
          service: portal
        annotations:
          summary: "Portal service is down"
          description: "The Portal (Next.js) service has been down for more than 2 minutes."

  # Node/System Alerts
  - name: node_alerts
    interval: 30s
    rules:
      # High CPU Usage
      - alert: HighCPUUsage
        expr: 100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
          service: system
        annotations:
          summary: "High CPU usage detected"
          description: "CPU usage is above 80% for more than 5 minutes on {{ $labels.instance }}."

      # High Memory Usage
      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: warning
          service: system
        annotations:
          summary: "High memory usage detected"
          description: "Memory usage is above 85% for more than 5 minutes on {{ $labels.instance }}."

      # Low Disk Space
      - alert: LowDiskSpace
        expr: (1 - (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"})) * 100 > 85
        for: 5m
        labels:
          severity: warning
          service: system
        annotations:
          summary: "Low disk space detected"
          description: "Disk usage is above 85% on {{ $labels.instance }}."

      # Node Exporter Down
      - alert: NodeExporterDown
        expr: up{job="node-exporter"} == 0
        for: 2m
        labels:
          severity: warning
          service: monitoring
        annotations:
          summary: "Node Exporter is down"
          description: "Node Exporter has been down for more than 2 minutes. System metrics unavailable."

  # Database Alerts (if using postgres_exporter)
  # - name: database_alerts
  #   interval: 30s
  #   rules:
  #     - alert: PostgreSQLDown
  #       expr: up{job="postgres"} == 0
  #       for: 2m
  #       labels:
  #         severity: critical
  #         service: database
  #       annotations:
  #         summary: "PostgreSQL is down"
  #         description: "PostgreSQL database has been down for more than 2 minutes."
  #
  #     - alert: HighDatabaseConnections
  #       expr: sum(pg_stat_activity_count) > 80
  #       for: 5m
  #       labels:
  #         severity: warning
  #         service: database
  #       annotations:
  #         summary: "High database connection count"
  #         description: "Database has more than 80 active connections."

  # Redis Alerts (if using redis_exporter)
  # - name: redis_alerts
  #   interval: 30s
  #   rules:
  #     - alert: RedisDown
  #       expr: up{job="redis"} == 0
  #       for: 2m
  #       labels:
  #         severity: critical
  #         service: cache
  #       annotations:
  #         summary: "Redis is down"
  #         description: "Redis cache has been down for more than 2 minutes."
  #
  #     - alert: HighRedisMemoryUsage
  #       expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.9
  #       for: 5m
  #       labels:
  #         severity: warning
  #         service: cache
  #       annotations:
  #         summary: "High Redis memory usage"
  #         description: "Redis memory usage is above 90%."

  # Monitoring Stack Alerts
  - name: monitoring_alerts
    interval: 60s
    rules:
      # Prometheus Down
      - alert: PrometheusDown
        expr: up{job="prometheus"} == 0
        for: 2m
        labels:
          severity: critical
          service: monitoring
        annotations:
          summary: "Prometheus is down"
          description: "Prometheus monitoring service is down."

      # Too Many Failed Scrapes
      - alert: HighScrapeFail Rate
        expr: rate(prometheus_target_scrapes_exceeded_sample_limit_total[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
          service: monitoring
        annotations:
          summary: "High scrape failure rate"
          description: "Prometheus is experiencing high scrape failure rate."
